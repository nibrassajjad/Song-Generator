{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81739,"sourceType":"datasetVersion","datasetId":6776}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Preprocessing the dataset**\n<br>Preprocessing steps are essential for preparing the text data for analysis or model training. Following preprocessing steps are done:\n1. Converting to lowercase: Converting text to lowercase ensures uniformity, so words like \"Hello\" and \"hello\" are treated the same. This reduces the complexity of the data by normalizing variations due to case differences.\n2. Replacing new lines with sapces: Replacing newlines with spaces ensures that sentences or words separated by newlines are still treated as distinct words. It also makes the text a single continuous block, which is easier to process for many natural language processing tasks.\n3. Removing carraige returns: Carriage returns (\\r) are often artifacts of different operating systems' text encoding formats. Removing them ensures consistency in the text format.\n4. Removing extra spaces: Multiple spaces can appear due to text replacements or formatting issues. Removing extra spaces ensures that words are separated by a single space, which is important for tasks like tokenization.\n5. Remove text within square brackets: Use regular expressions to remove any content within square brackets, such as [chorus:].","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\n\n# Load the lyrics file for the chosen artist\nfile_path = '/kaggle/input/poetry/michael-jackson.txt'  # Load which artist/band file you want to train upon\nwith open(file_path, 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# Preprocessing the text\ntext = text.lower()  # Convert to lowercase\ntext = text.replace('\\n', ' ')  # Replace newlines with spaces\ntext = text.replace('\\r', '')  # Remove carriage returns\ntext = re.sub(' +', ' ', text)  # Remove extra spaces\ntext = re.sub(r'\\[.*?\\]', '', text)  # Remove text within square brackets\n\nprint(f\"Corpus length: {len(text)}\")\n\n# Tokenize the text into words\nwords = text.split()\n\n# Select the first 50 words\nfirst_50_words = words[:50]\n\n# Join these words into a single string\nfirst_50_words_text = ' '.join(first_50_words)\n\nprint(f\"\\nFirst 50 words of the corpus: \\n{first_50_words_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:16:55.911969Z","iopub.execute_input":"2024-08-03T12:16:55.912352Z","iopub.status.idle":"2024-08-03T12:16:55.952237Z","shell.execute_reply.started":"2024-08-03T12:16:55.912322Z","shell.execute_reply":"2024-08-03T12:16:55.950997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data exploration**\n<br>The corpus is explored to see the most common words.\n<br>Note: Stopwords set used by the WordCloud library includes many common English words such as \"you\". These stopwords are excluded from the word cloud to focus on more meaningful and unique words.","metadata":{}},{"cell_type":"code","source":"# THIS CELL IS ONLY IF YOU WANT TO DISCARD ANY STOPWORD\n\"\"\"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Print the default stopwords\nprint(\"Default stopwords:\\n\", STOPWORDS)\n\n# Modify stopwords to exclude specific words\nstopwords = set(STOPWORDS)\nstopwords.discard('you')  # Ensure 'you' is not treated as a stopword\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n\n# Count the frequency of each word in the entire corpus\nword_counts = Counter(words)\n\n# Display the 10 most common words and their counts\nmost_common_words = word_counts.most_common(10)\nprint(\"\\nMost common words in the corpus:\")\nfor word, count in most_common_words:\n    print(f\"{word}: {count}\")\n\n# Display the frequency of a specific word (optional)\nspecific_word = 'love'  # Example word\nprint(f\"\\nFrequency of the word '{specific_word}': {word_counts[specific_word]}\")\n\n# Generate a word cloud image\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", max_words=800).generate(\" \".join(words))\n\n# Display the word cloud\nplt.figure(figsize=(5, 5))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:16:57.737457Z","iopub.execute_input":"2024-08-03T12:16:57.737835Z","iopub.status.idle":"2024-08-03T12:16:58.386036Z","shell.execute_reply.started":"2024-08-03T12:16:57.737803Z","shell.execute_reply":"2024-08-03T12:16:58.384825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Character Mappings**\n<br>Create mappings from characters to integers and vice versa.\n<br><br>In word-level models, any word not present in the training vocabulary is treated as unknown or out-of-vocabulary (OOV). With character-level models, every word is broken down into characters, which means you can handle any new or rare words simply by processing their characters. With characters, the vocabulary size is limited to the character set, which is usually smaller and more manageable.","metadata":{}},{"cell_type":"code","source":"# Create a mapping from unique characters to indices\n\n# Specify characters to remove\nchars_to_remove = {'@', '#', '$'}  # Example: remove @, #, $\n\n# Create a new text excluding the specified characters\nfiltered_text = ''.join(char for char in text if char not in chars_to_remove)\n\n# Create a new mapping from unique characters to indices\nchars = sorted(list(set(filtered_text)))\nchar_to_idx = {char: idx for idx, char in enumerate(chars)}\nidx_to_char = {idx: char for idx, char in enumerate(chars)}\n\nprint(f\"Original corpus length: {len(text)}\")\nprint(f\"Filtered corpus length: {len(filtered_text)}\")\n\nprint(f\"Unique characters after filtering: {len(chars)}\")\nprint(chars)","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:31:09.061778Z","iopub.execute_input":"2024-08-03T12:31:09.062252Z","iopub.status.idle":"2024-08-03T12:31:09.110558Z","shell.execute_reply.started":"2024-08-03T12:31:09.062222Z","shell.execute_reply":"2024-08-03T12:31:09.109441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Create Training Data**\n<br>Sequence of characters can be created to be used as input and the corresponding next character as the output","metadata":{}},{"cell_type":"code","source":"# Create sequences and next character targets from filtered_text\nmaxlen = 40  # Length of each sequence\nstep = 3  # Step size to create new sequences\n\nsequences = []\nnext_chars = []\n\nfor i in range(0, len(filtered_text) - maxlen, step):\n    sequences.append(filtered_text[i: i + maxlen])\n    next_chars.append(filtered_text[i + maxlen])\n\nprint(f\"Number of sequences: {len(sequences)}\")\n\n# Vectorize the sequences\nX = np.zeros((len(sequences), maxlen, len(chars)), dtype=bool)\ny = np.zeros((len(sequences), len(chars)), dtype=bool)\n\nfor i, seq in enumerate(sequences):\n    for t, char in enumerate(seq):\n        if char in char_to_idx:  # Check if character is in the mapping\n            X[i, t, char_to_idx[char]] = 1\n    if next_chars[i] in char_to_idx:  # Check if next_char is in the mapping\n        y[i, char_to_idx[next_chars[i]]] = 1\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:50:15.666911Z","iopub.execute_input":"2024-08-03T12:50:15.667343Z","iopub.status.idle":"2024-08-03T12:50:17.423769Z","shell.execute_reply.started":"2024-08-03T12:50:15.667286Z","shell.execute_reply":"2024-08-03T12:50:17.42263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Explanation:**\n<br>Sequences from the filtered_text with a length of maxlen (40 characters) and moving with a step size of 3.\n<br>For each starting position i, we extract a sequence of 40 characters and determine the next character.\n<br>The total number of possible sequences you can create is determined by the formula:\n$$ No. of Sequences  = \\frac{244044-40}{3} +1 = 81335$$\n<br>X is a 3D numpy array representing the input data for training the model. Shape of X is (number_of_sequences, maxlen, number_of_unique_characters).\n<br>number_of_sequences = 81335\n<br>maxlen = 40 (length of each sequence)\n<br>number_of_unique_characters = 54 (unique characters in filtered_text)\n<br><br>y is a 2D numpy array representing the target data for training the model.\n<br>Shape of y is (number_of_sequences, number_of_unique_characters).\n<br>number_of_sequences = 81335\n<br>number_of_unique_characters = 54 (unique characters in filtered_text)\n<br><br>For each sequence in X, y provides the one-hot encoded vector of the next character. Each entry in y is a vector of length 54, corresponding to the next character after each sequence.","metadata":{}},{"cell_type":"markdown","source":"**Build the Model**\n<br>LSTM layer is chosen for its ability to handle sequential dependencies, which is crucial for text generation. \n<br>Input shape of LSTM is defined as input_shape=(timesteps, features) where timesteps is the number of time steps in each sequence, while features is the number of features(or dimensionality) at each time step.\n<br><br>**Explanation of LSTM layer algorithm**\n<br><br>Example Text: \"hello world\"\n<br>Sequence Length (maxlen): 5 characters (for simplicity).\n<br>Unique Characters (features): We have 10 unique characters.\n<br>Sequences: ['hello', 'ello ', 'llo w', 'lo wo', 'o wor', ' worl', 'world']\n<br><br>Each sequence is represented as:\n<br>5 time steps (since maxlen is 5), 10 features (one-hot encoding for each character).\n<br>If you represent this in a numpy array for one sequence, it will look like this: (5, 10)\n<br>For all sequences combined:\n<br>(7, 5, 10) (where 7 is the number of sequences)\n\n<br>Dense layer at end has neurons = len(chars). The number of neurons in the Dense layer is equal to the number of unique characters. This allows the model to output a probability distribution over all possible characters.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars), activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T12:50:27.519366Z","iopub.execute_input":"2024-08-03T12:50:27.51977Z","iopub.status.idle":"2024-08-03T12:50:27.661653Z","shell.execute_reply.started":"2024-08-03T12:50:27.519735Z","shell.execute_reply":"2024-08-03T12:50:27.660574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Model**","metadata":{}},{"cell_type":"code","source":"model.fit(X, y, batch_size=128, epochs=20)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T13:40:46.523809Z","iopub.execute_input":"2024-08-03T13:40:46.52475Z","iopub.status.idle":"2024-08-03T13:54:08.590717Z","shell.execute_reply.started":"2024-08-03T13:40:46.524712Z","shell.execute_reply":"2024-08-03T13:54:08.589539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generate New Lyrics**","metadata":{}},{"cell_type":"code","source":"import random\n\ndef sample(preds, temperature=1.0):\n    # Helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n# Start with a random seed sequence\nstart_index = random.randint(0, len(text) - maxlen - 1)\nseed_text = text[start_index: start_index + maxlen]\nprint(f\"Seed text: {seed_text}\")\n\ngenerated_text = seed_text\n\nfor i in range(400):  # Generate 400 characters\n    sampled = np.zeros((1, maxlen, len(chars)))\n    for t, char in enumerate(seed_text):\n        sampled[0, t, char_to_idx[char]] = 1\n\n    preds = model.predict(sampled, verbose=0)[0]\n    next_index = sample(preds, temperature=0.5)\n    next_char = idx_to_char[next_index]\n\n    generated_text += next_char\n    seed_text = seed_text[1:] + next_char\n\nprint(f\"Generated text: {generated_text}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-03T13:54:54.845388Z","iopub.execute_input":"2024-08-03T13:54:54.84579Z","iopub.status.idle":"2024-08-03T13:55:27.388621Z","shell.execute_reply.started":"2024-08-03T13:54:54.845757Z","shell.execute_reply":"2024-08-03T13:55:27.38746Z"},"trusted":true},"execution_count":null,"outputs":[]}]}