{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session \n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the dataset**\n<br>Preprocessing steps are essential for preparing the text data for analysis or model training. Following preprocessing steps are done:\n1. Converting to lowercase: Converting text to lowercase ensures uniformity, so words like \"Hello\" and \"hello\" are treated the same. This reduces the complexity of the data by normalizing variations due to case differences.\n2. Replacing new lines with sapces: Replacing newlines with spaces ensures that sentences or words separated by newlines are still treated as distinct words. It also makes the text a single continuous block, which is easier to process for many natural language processing tasks.\n3. Removing carraige returns: Carriage returns (\\r) are often artifacts of different operating systems' text encoding formats. Removing them ensures consistency in the text format.\n4. Removing extra spaces: Multiple spaces can appear due to text replacements or formatting issues. Removing extra spaces ensures that words are separated by a single space, which is important for tasks like tokenization.\n5. Remove text within square brackets: Use regular expressions to remove any content within square brackets, such as [chorus:].","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re # Imports regular expressions library\n\n# Load the lyrics file for the chosen artist\nfile_path = '/kaggle/input/poetry/michael-jackson.txt'  # Load which artist/band file you want to train upon\nwith open(file_path, 'r', encoding='utf-8') as file:\n    text = file.read()\n\n# Preprocessing the text\ntext = text.lower()  # Convert to lowercase\ntext = text.replace('\\n', ' ')  # Replace newlines with spaces\ntext = text.replace('\\r', '')  # Remove carriage returns\ntext = re.sub(' +', ' ', text)  # Remove extra spaces\ntext = re.sub(r'\\[.*?\\]', '', text)  # Remove text within square brackets\n\nprint(f\"Corpus length: {len(text)}\")\n\n# Tokenize the text into words\nwords = text.split()\n\n# Select the first 50 words\nfirst_50_words = words[:50]\n\n# Join these words into a single string\nfirst_50_words_text = ' '.join(first_50_words)\n\nprint(f\"\\nFirst 50 words of the corpus: \\n{first_50_words_text}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T15:06:15.465896Z","iopub.execute_input":"2024-08-01T15:06:15.466819Z","iopub.status.idle":"2024-08-01T15:06:15.506465Z","shell.execute_reply.started":"2024-08-01T15:06:15.466785Z","shell.execute_reply":"2024-08-01T15:06:15.505334Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Corpus length: 244046\n\nFirst 50 words of the corpus: \nyou ever want something that you know you shouldn't have the more you know you shouldn't have it, the more you want it and then one day you get it, it's so good too but it's just like my girl when she's around me i just feel so good, so\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create Character Mappings**\n<br>Create mappings from characters to integers and vice versa.\n<br><br>In word-level models, any word not present in the training vocabulary is treated as unknown or out-of-vocabulary (OOV). With character-level models, every word is broken down into characters, which means you can handle any new or rare words simply by processing their characters. With characters, the vocabulary size is limited to the character set, which is usually smaller and more manageable.","metadata":{}},{"cell_type":"code","source":"# Create a mapping from unique characters to indices\n\n# Specify characters to remove\nchars_to_remove = {'@', '#', '$'}  # Example: remove @, #, $\n\n# Create a new text excluding the specified characters\nfiltered_text = ''.join(char for char in text if char not in chars_to_remove)\n\n# Create a new mapping from unique characters to indices\nchars = sorted(list(set(filtered_text)))\nchar_to_idx = {char: idx for idx, char in enumerate(chars)}\nidx_to_char = {idx: char for idx, char in enumerate(chars)}\n\nprint(f\"Original corpus length: {len(text)}\")\nprint(f\"Filtered corpus length: {len(filtered_text)}\")\n\nprint(f\"Unique characters after filtering: {len(chars)}\")\nprint(chars)","metadata":{"execution":{"iopub.status.busy":"2024-08-01T15:44:28.934241Z","iopub.execute_input":"2024-08-01T15:44:28.934612Z","iopub.status.idle":"2024-08-01T15:44:28.979288Z","shell.execute_reply.started":"2024-08-01T15:44:28.934586Z","shell.execute_reply":"2024-08-01T15:44:28.977978Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Original corpus length: 244046\nFiltered corpus length: 244044\nUnique characters after filtering: 54\n[' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '8', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '´', 'â', 'è', 'é', 'ê', '‘', '’', '“', '”', '…']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Create Training Data**\n<br>Sequence of characters can be created to be used as input and the corresponding next character as the output","metadata":{}},{"cell_type":"code","source":"# Create sequences and next character targets from filtered_text\nmaxlen = 40  # Length of each sequence\nstep = 3  # Step size to create new sequences\n\nsequences = []\nnext_chars = []\n\nfor i in range(0, len(filtered_text) - maxlen, step):\n    sequences.append(filtered_text[i: i + maxlen])\n    next_chars.append(filtered_text[i + maxlen])\n\nprint(f\"Number of sequences: {len(sequences)}\")\n\n# Vectorize the sequences\nX = np.zeros((len(sequences), maxlen, len(chars)), dtype=bool)\ny = np.zeros((len(sequences), len(chars)), dtype=bool)\n\nfor i, seq in enumerate(sequences):\n    for t, char in enumerate(seq):\n        if char in char_to_idx:  # Check if character is in the mapping\n            X[i, t, char_to_idx[char]] = 1\n    if next_chars[i] in char_to_idx:  # Check if next_char is in the mapping\n        y[i, char_to_idx[next_chars[i]]] = 1\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T15:47:56.030224Z","iopub.execute_input":"2024-08-01T15:47:56.030757Z","iopub.status.idle":"2024-08-01T15:47:57.945350Z","shell.execute_reply.started":"2024-08-01T15:47:56.030712Z","shell.execute_reply":"2024-08-01T15:47:57.944226Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Number of sequences: 81335\nShape of X: (81335, 40, 54)\nShape of y: (81335, 54)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Build the Model**","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense\n\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars), activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Train the Model**","metadata":{}},{"cell_type":"code","source":"model.fit(X, y, batch_size=128, epochs=20)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generate New Lyrics**","metadata":{}},{"cell_type":"code","source":"import random\n\ndef sample(preds, temperature=1.0):\n    # Helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n# Start with a random seed sequence\nstart_index = random.randint(0, len(text) - maxlen - 1)\nseed_text = text[start_index: start_index + maxlen]\nprint(f\"Seed text: {seed_text}\")\n\ngenerated_text = seed_text\n\nfor i in range(400):  # Generate 400 characters\n    sampled = np.zeros((1, maxlen, len(chars)))\n    for t, char in enumerate(seed_text):\n        sampled[0, t, char_to_idx[char]] = 1\n\n    preds = model.predict(sampled, verbose=0)[0]\n    next_index = sample(preds, temperature=0.5)\n    next_char = idx_to_char[next_index]\n\n    generated_text += next_char\n    seed_text = seed_text[1:] + next_char\n\nprint(f\"Generated text: {generated_text}\")\n","metadata":{},"execution_count":null,"outputs":[]}]}